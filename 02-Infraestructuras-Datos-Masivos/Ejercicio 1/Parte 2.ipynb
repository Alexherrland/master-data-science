{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alex Herrerías Ramírez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "Creamos el directorio donde vamos a realizar el ejercicio y nos aseguramos de estar en ese directorio\n",
    "\n",
    "Nota adicional: Se usara en las consultas a beehive \"2>&1 | grep -vE \"SLF4J|WARN\"\" para eliminar mensajes innecesarios y mejorar la claridad del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks\n"
     ]
    }
   ],
   "source": [
    "# En el caso de que se ejecute varias veces el codigo, asegurarse estar en la carpeta de inicio\n",
    "import os\n",
    "os.chdir(\"/media/notebooks\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/ejercicio-2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "!mkdir -p ejercicio-2\n",
    "os.chdir(\"ejercicio-2\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = 84304261-7a93-40bb-8fc1-126edfe92268\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "No rows affected (1.282 seconds)\n",
      "No rows affected (0.076 seconds)\n",
      "No rows affected (0.05 seconds)\n",
      "Beeline version 4.0.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://\n"
     ]
    }
   ],
   "source": [
    "#En el caso de que no sea la primera vez que se ejecute el codigo, para borrar las tablas ya existentes\n",
    "!beeline -u \"jdbc:hive2://\" -e \"\\\n",
    "DROP TABLE IF EXISTS convocatorias_2020; \\\n",
    "DROP TABLE IF EXISTS steam_spy_data; \\\n",
    "DROP TABLE IF EXISTS metacritic_games;\" 2>&1 | grep -vE \"SLF4J|WARN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1: Subir datos a HDFS\n",
    "Se asume que el archivo \"convocatorias-2020\" esta en /media/notebooks/ejercicio-2 (en la carpeta recien creada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/ejercicio-2/convocatorias-2020.csv': File exists\n",
      "-rw-r--r--   3 root supergroup    5675992 2025-11-20 10:11 /ejercicio-2/convocatorias-2020.csv\n",
      "-rw-r--r--   3 root supergroup    5676272 2025-11-19 15:19 /ejercicio-2/convocatorias-2020_copy_1.csv\n",
      "-rw-r--r--   3 root supergroup    5675992 2025-11-19 15:37 /ejercicio-2/convocatorias-2020_copy_2.csv\n",
      "-rw-r--r--   3 root supergroup    5675992 2025-11-20 09:59 /ejercicio-2/convocatorias-2020_copy_3.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir -p /ejercicio-2\n",
    "! hdfs dfs -put /media/notebooks/ejercicio-2/convocatorias-2020.csv /ejercicio-2/\n",
    "!hdfs dfs -ls -R /ejercicio-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2: Creación de la Tabla en Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siguiendo las practicas en los apuntes de la asignatura, se ha creado una tabla externa ya que los datos ya existen en HDFS de forma independiente a hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting crear-tabla-convocatorias.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile crear-tabla-convocatorias.hql\n",
    "CREATE EXTERNAL TABLE convocatorias_2020 (\n",
    "    numero_procedimiento STRING,\n",
    "    nro_saf INT,\n",
    "    descripcion_saf STRING,\n",
    "    nro_uoc INT,\n",
    "    descripcion_uoc STRING,\n",
    "    tipo_procedimiento STRING,\n",
    "    modalidad STRING,\n",
    "    apartado_directa STRING,\n",
    "    ejercicio INT,\n",
    "    fecha_publicacion STRING,\n",
    "    fecha_apertura STRING,\n",
    "    etapa STRING,\n",
    "    alcance STRING,\n",
    "    nombre_procedimiento STRING,\n",
    "    objeto_procedimiento STRING,\n",
    "    monto_estimado DOUBLE,\n",
    "    tipo_operacion STRING\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/ejercicio-2';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos el comando en beeline para la creación del archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = a6591e18-4640-46b0-bbb4-d63e622d0f28\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "0: jdbc:hive2://> CREATE EXTERNAL TABLE convocatorias_2020 (\n",
      ". . . . . . . . >     numero_procedimiento STRING,\n",
      ". . . . . . . . >     nro_saf INT,\n",
      ". . . . . . . . >     descripcion_saf STRING,\n",
      ". . . . . . . . >     nro_uoc INT,\n",
      ". . . . . . . . >     descripcion_uoc STRING,\n",
      ". . . . . . . . >     tipo_procedimiento STRING,\n",
      ". . . . . . . . >     modalidad STRING,\n",
      ". . . . . . . . >     apartado_directa STRING,\n",
      ". . . . . . . . >     ejercicio INT,\n",
      ". . . . . . . . >     fecha_publicacion STRING,\n",
      ". . . . . . . . >     fecha_apertura STRING,\n",
      ". . . . . . . . >     etapa STRING,\n",
      ". . . . . . . . >     alcance STRING,\n",
      ". . . . . . . . >     nombre_procedimiento STRING,\n",
      ". . . . . . . . >     objeto_procedimiento STRING,\n",
      ". . . . . . . . >     monto_estimado DOUBLE,\n",
      ". . . . . . . . >     tipo_operacion STRING\n",
      ". . . . . . . . > )\n",
      ". . . . . . . . > ROW FORMAT DELIMITED\n",
      ". . . . . . . . > FIELDS TERMINATED BY ','\n",
      ". . . . . . . . > STORED AS TEXTFILE\n",
      ". . . . . . . . > LOCATION '/ejercicio-2';\n",
      "No rows affected (1.128 seconds)\n",
      "0: jdbc:hive2://> \n",
      "0: jdbc:hive2://> Closing: 0: jdbc:hive2://\n"
     ]
    }
   ],
   "source": [
    "!beeline -u \"jdbc:hive2://\" -f crear-tabla-convocatorias.hql 2>&1 | grep -vE \"SLF4J|WARN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos del archivo csv dentro de la tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = 5f76fd6d-863c-419e-b9b5-404ad0ccf5d3\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "Loading data to table default.convocatorias_2020\n",
      "No rows affected (1.186 seconds)\n",
      "Beeline version 4.0.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://\n"
     ]
    }
   ],
   "source": [
    "! beeline -u \"jdbc:hive2://\" -e \\\n",
    "\"LOAD DATA INPATH '/ejercicio-2/convocatorias-2020.csv' \\\n",
    "INTO TABLE convocatorias_2020;\" 2>&1 | grep -vE \"SLF4J|WARN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que se han cargado los datos correctamente haciendo un select de las 3 primeras filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = 0e0ac8f0-bf17-4ec3-b39f-5f76d69e1b16\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "No Stats for default@convocatorias_2020, Columns: nombre_procedimiento, descripcion_uoc, etapa, fecha_apertura, nro_uoc, objeto_procedimiento, tipo_procedimiento, ejercicio, fecha_publicacion, alcance, descripcion_saf, modalidad, monto_estimado, numero_procedimiento, nro_saf, apartado_directa, tipo_operacion\n",
      "+------------------------------------------+-----------------------------+----------------------------------------------------+-----------------------------+----------------------------------------------------+----------------------------------------+-------------------------------+-------------------------------------------+-------------------------------+---------------------------------------+------------------------------------+---------------------------+-----------------------------+----------------------------------------------------+----------------------------------------------------+------------------------------------+------------------------------------+\n",
      "| convocatorias_2020.numero_procedimiento  | convocatorias_2020.nro_saf  |         convocatorias_2020.descripcion_saf         | convocatorias_2020.nro_uoc  |         convocatorias_2020.descripcion_uoc         | convocatorias_2020.tipo_procedimiento  | convocatorias_2020.modalidad  |    convocatorias_2020.apartado_directa    | convocatorias_2020.ejercicio  | convocatorias_2020.fecha_publicacion  | convocatorias_2020.fecha_apertura  | convocatorias_2020.etapa  | convocatorias_2020.alcance  |      convocatorias_2020.nombre_procedimiento       |      convocatorias_2020.objeto_procedimiento       | convocatorias_2020.monto_estimado  | convocatorias_2020.tipo_operacion  |\n",
      "+------------------------------------------+-----------------------------+----------------------------------------------------+-----------------------------+----------------------------------------------------+----------------------------------------+-------------------------------+-------------------------------------------+-------------------------------+---------------------------------------+------------------------------------+---------------------------+-----------------------------+----------------------------------------------------+----------------------------------------------------+------------------------------------+------------------------------------+\n",
      "| N?mero Procedimiento                     | NULL                        | Descripcion SAF                                    | NULL                        | Descripcion UOC                                    | Tipo de Procedimiento                  | Modalidad                     | Apartado Directa                          | NULL                          | Fecha de Publicaci?n                  | Fecha de Apertura                  | Etapa                     | Alcance                     | Nombre del Procedimiento                           | Objeto del Procedimiento                           | NULL                               | Tipo de Operaci?n                  |\n",
      "| 40/51-0159-LPR19                         | 381                         | 381 - Estado Mayor General de La Fuerza A?rea      | 40                          | Departamento Contrataciones C?rdoba - UOC 40/51    | Licitacion Privada                     | Sin Modalidad                 |                                           | 2019                          | 02/01/2020 08:00:00 a.m.              | 10/01/2020 10:00:00 a.m.           | ?nica                     | Nacional                    | ?Adquisici?n de Elementos de Librer?a para el HOSPITAL AERON?UTICO C?RDOBA | ?Adquisici?n de Elementos de Librer?a para el HOSPITAL AERON?UTICO C?RDOBA | 1957142.84                         | Proceso de Compra                  |\n",
      "| 22-0041-CDI19                            | 250                         | 250 - Caja de Retiros Jubilaciones y Pensiones de la Polic?a Federal | 22                          | 22 - Dpto de Compras y Suministros - Caja de Retiros Jubilaciones y Pensiones de la Polic?a Federal | Contrataci?n Directa                   | Sin Modalidad                 | Apartado 1: Compulsa Abreviada Por Monto  | 2019                          | 02/01/2020 12:00:00 p.m.              | 13/01/2020 09:00:00 a.m.           | ?nica                     | Nacional                    | Servicio de seguridad de informaci?n sensible de la p?gina web del Organismo. | Servicio de seguridad de informaci?n sensible de la p?gina web del Organismo. | 119915.25                          | Proceso de Compra                  |\n",
      "+------------------------------------------+-----------------------------+----------------------------------------------------+-----------------------------+----------------------------------------------------+----------------------------------------+-------------------------------+-------------------------------------------+-------------------------------+---------------------------------------+------------------------------------+---------------------------+-----------------------------+----------------------------------------------------+----------------------------------------------------+------------------------------------+------------------------------------+\n",
      "3 rows selected (2.236 seconds)\n",
      "Beeline version 4.0.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://\n"
     ]
    }
   ],
   "source": [
    "! beeline -u \"jdbc:hive2://\" -e \"SELECT * FROM convocatorias_2020 LIMIT 3;\" 2>&1 | grep -vE \"SLF4J|WARN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 3: Consultas HiveQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Contratación Directa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = 8368388d-ec18-4e77-b8e6-5bd05c0de3c7\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "No Stats for default@convocatorias_2020, Columns: numero_procedimiento, tipo_procedimiento\n",
      "+-----------------------+-----------------------+\n",
      "| numero_procedimiento  |  tipo_procedimiento   |\n",
      "+-----------------------+-----------------------+\n",
      "| 22-0041-CDI19         | Contrataci?n Directa  |\n",
      "| 40/51-0918-CDI19      | Contrataci?n Directa  |\n",
      "| 92-0075-CDI19         | Contrataci?n Directa  |\n",
      "| 46/1-0005-CDI20       | Contrataci?n Directa  |\n",
      "| 92-0077-CDI19         | Contrataci?n Directa  |\n",
      "+-----------------------+-----------------------+\n",
      "5 rows selected (2.165 seconds)\n",
      "Beeline version 4.0.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://\n"
     ]
    }
   ],
   "source": [
    "!beeline -u \"jdbc:hive2://\" -e \\\n",
    "\"SELECT numero_procedimiento, tipo_procedimiento FROM convocatorias_2020 WHERE lower(tipo_procedimiento) LIKE '%contrataci%n directa%' LIMIT 5;\" 2>&1 | grep -vE \"SLF4J|WARN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Contratación Directa anterior a 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = 0a8a795f-de01-41ee-8a02-1d711ef8a95f\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "No Stats for default@convocatorias_2020, Columns: numero_procedimiento, tipo_procedimiento, ejercicio\n",
      "+-----------------------+------------+\n",
      "| numero_procedimiento  | ejercicio  |\n",
      "+-----------------------+------------+\n",
      "| 22-0041-CDI19         | 2019       |\n",
      "| 40/51-0918-CDI19      | 2019       |\n",
      "| 92-0075-CDI19         | 2019       |\n",
      "| 92-0077-CDI19         | 2019       |\n",
      "| 41-0021-CDI19         | 2019       |\n",
      "+-----------------------+------------+\n",
      "5 rows selected (2.251 seconds)\n",
      "Beeline version 4.0.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://\n"
     ]
    }
   ],
   "source": [
    "!beeline -u \"jdbc:hive2://\" -e \\\n",
    "\"SELECT numero_procedimiento, ejercicio FROM convocatorias_2020 WHERE lower(tipo_procedimiento) LIKE '%contrataci%n directa%' AND CAST(ejercicio AS INT) < 2020 LIMIT 5;\" 2>&1 | grep -vE \"SLF4J|WARN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Tipos de SAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = c3ae1166-9f84-489d-a8f0-28ab51fb537a\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "No Stats for default@convocatorias_2020, Columns: descripcion_saf\n",
      "Query ID = root_20251120101252_39aa9a44-efb5-4fc1-bd3a-07843679b3ae\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1763631857148_0009, Tracking URL = http://yarnmanager:8088/proxy/application_1763631857148_0009/\n",
      "Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1763631857148_0009\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2025-11-20 10:13:01,303 Stage-1 map = 0%,  reduce = 0%\n",
      "2025-11-20 10:13:06,476 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.63 sec\n",
      "2025-11-20 10:13:12,583 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.87 sec\n",
      "MapReduce Total cumulative CPU time: 4 seconds 870 msec\n",
      "Ended Job = job_1763631857148_0009\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.87 sec   HDFS Read: 22723587 HDFS Write: 103 HDFS EC Read: 0 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 4 seconds 870 msec\n",
      "+------+\n",
      "| _c0  |\n",
      "+------+\n",
      "| 126  |\n",
      "+------+\n",
      "1 row selected (20.943 seconds)\n",
      "Beeline version 4.0.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://\n"
     ]
    }
   ],
   "source": [
    "!beeline -u \"jdbc:hive2://\" -e \\\n",
    "\"SELECT count(DISTINCT descripcion_saf) FROM convocatorias_2020;\" 2>&1 | grep -vE \"SLF4J|WARN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Máximo Presupuesto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = b9db3a2a-434e-4543-bca0-b76f28ee820e\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "No Stats for default@convocatorias_2020, Columns: monto_estimado\n",
      "Query ID = root_20251120101318_462088c9-a291-4c46-b2a2-c1e3041b154d\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1763631857148_0010, Tracking URL = http://yarnmanager:8088/proxy/application_1763631857148_0010/\n",
      "Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1763631857148_0010\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2025-11-20 10:13:26,574 Stage-1 map = 0%,  reduce = 0%\n",
      "2025-11-20 10:13:30,659 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.47 sec\n",
      "2025-11-20 10:13:36,764 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.81 sec\n",
      "MapReduce Total cumulative CPU time: 4 seconds 810 msec\n",
      "Ended Job = job_1763631857148_0010\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 4.81 sec   HDFS Read: 22723137 HDFS Write: 109 HDFS EC Read: 0 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 4 seconds 810 msec\n",
      "+------------+\n",
      "|    _c0     |\n",
      "+------------+\n",
      "| 8.28816E9  |\n",
      "+------------+\n",
      "1 row selected (19.97 seconds)\n",
      "Beeline version 4.0.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://\n"
     ]
    }
   ],
   "source": [
    "!beeline -u \"jdbc:hive2://\" -e \\\n",
    "\"SELECT MAX(monto_estimado) FROM convocatorias_2020;\" 2>&1 | grep -vE \"SLF4J|WARN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Promedio por Alcance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hive Session ID = 2988c1c1-e9ce-43c7-a34a-492297958528\n",
      "No Stats for default@convocatorias_2020, Columns: monto_estimado, alcance\n",
      "Query ID = root_20251120101344_fb15d158-912b-4cf2-a010-ce323a667261\n",
      "Total jobs = 1\n",
      "Launching Job 1 out of 1\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1763631857148_0011, Tracking URL = http://yarnmanager:8088/proxy/application_1763631857148_0011/\n",
      "Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1763631857148_0011\n",
      "Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\n",
      "2025-11-20 10:13:52,623 Stage-1 map = 0%,  reduce = 0%\n",
      "2025-11-20 10:13:56,715 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.74 sec\n",
      "2025-11-20 10:14:02,809 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.97 sec\n",
      "MapReduce Total cumulative CPU time: 5 seconds 970 msec\n",
      "Ended Job = job_1763631857148_0011\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 5.97 sec   HDFS Read: 22725559 HDFS Write: 196 HDFS EC Read: 0 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 5 seconds 970 msec\n",
      "+----------------+-----------------------+\n",
      "|    alcance     |          _c1          |\n",
      "+----------------+-----------------------+\n",
      "| Alcance        | NULL                  |\n",
      "| Internacional  | 1.2112060488888891E7  |\n",
      "| Nacional       | 8836399.135253673     |\n",
      "+----------------+-----------------------+\n"
     ]
    }
   ],
   "source": [
    "!beeline -u \"jdbc:hive2://\" --silent=true -e \\\n",
    "\"SELECT alcance, AVG(monto_estimado) FROM convocatorias_2020 GROUP BY alcance;\" 2>&1 | grep -vE \"SLF4J|WARN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Dataset Extra\n",
    "Fuente de Datos:\n",
    "\n",
    "- Nombre: Top PC Games: Metacritic vs Steam Popularity\n",
    "\n",
    "- Origen: Kaggle (https://www.kaggle.com/datasets/alyahmedts13/top-pc-games-metacritic-vs-steam-popularity)\n",
    "\n",
    "- Contiene dos ficheros csv, uno contiene la información de las ventas y jugadores de Steam (steam_spy_data_clean.csv) y el otro las puntuaciones y críticas de Metacritic (metacritic_Toppc_games_clean.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siguiendo la metodologia de los apuntes, creamos un directorio para almacenar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/videojuegos/steam/steam_spy_data_clean.csv': File exists\n",
      "put: `/videojuegos/metacritic/metacritic_Toppc_games_clean.csv': File exists\n",
      "drwxr-xr-x   - root supergroup          0 2025-11-19 15:31 /videojuegos/metacritic\n",
      "-rw-r--r--   3 root supergroup    5675992 2025-11-19 15:30 /videojuegos/metacritic/convocatorias-2020.csv\n",
      "-rw-r--r--   3 root supergroup    3699675 2025-11-19 15:31 /videojuegos/metacritic/metacritic_Toppc_games_clean.csv\n",
      "drwxr-xr-x   - root supergroup          0 2025-11-19 15:31 /videojuegos/steam\n",
      "-rw-r--r--   3 root supergroup    5675992 2025-11-19 15:30 /videojuegos/steam/convocatorias-2020.csv\n",
      "-rw-r--r--   3 root supergroup    1292579 2025-11-19 15:31 /videojuegos/steam/steam_spy_data_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# Crear directorios separados en HDFS\n",
    "!hdfs dfs -mkdir -p /videojuegos/steam\n",
    "!hdfs dfs -mkdir -p /videojuegos/metacritic\n",
    "\n",
    "# Subir los archivos\n",
    "! hdfs dfs -put /media/notebooks/ejercicio-2/steam_spy_data_clean.csv /videojuegos/steam/\n",
    "! hdfs dfs -put /media/notebooks/ejercicio-2/metacritic_Toppc_games_clean.csv /videojuegos/metacritic/\n",
    "!hdfs dfs -ls -R /videojuegos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la tabla de forma externa al igual que en el anterior ejercicio con los datos localizados en hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting crear-tablas-videojuegos.hql\n"
     ]
    }
   ],
   "source": [
    "%%writefile crear-tablas-videojuegos.hql\n",
    "\n",
    "-- Tabla 1: Datos de Steam\n",
    "CREATE EXTERNAL TABLE steam_spy_data (\n",
    "    appid INT,\n",
    "    name STRING,\n",
    "    developer STRING,\n",
    "    publisher STRING,\n",
    "    positive INT,\n",
    "    negative INT,\n",
    "    owners_range STRING,\n",
    "    average_playtime_total INT,\n",
    "    average_playtime_2weeks INT,\n",
    "    median_playtime_total INT,\n",
    "    median_playtime_2weeks INT,\n",
    "    price_usd DOUBLE,\n",
    "    initialprice_usd DOUBLE,\n",
    "    discount_percent DOUBLE,\n",
    "    peak_current_players_yesterday INT,\n",
    "    estimated_owners BIGINT\n",
    ")\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "WITH SERDEPROPERTIES (\n",
    "   \"separatorChar\" = \",\",\n",
    "   \"quoteChar\"     = \"\\\"\",\n",
    "   \"escapeChar\"    = \"\\\\\"\n",
    ")\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/videojuegos/steam'\n",
    "TBLPROPERTIES ('skip.header.line.count'='1');\n",
    "\n",
    "-- Tabla 2: Datos de Metacritic\n",
    "CREATE EXTERNAL TABLE metacritic_games (\n",
    "    name STRING,\n",
    "    release_date STRING,\n",
    "    rating STRING,\n",
    "    description STRING,\n",
    "    score INT\n",
    ")\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "WITH SERDEPROPERTIES (\n",
    "   \"separatorChar\" = \",\",\n",
    "   \"quoteChar\"     = \"\\\"\",\n",
    "   \"escapeChar\"    = \"\\\\\"\n",
    ")\n",
    "STORED AS TEXTFILE\n",
    "LOCATION '/videojuegos/metacritic'\n",
    "TBLPROPERTIES ('skip.header.line.count'='1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos el comando para crear la tabla en hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = d90bbb51-bce5-4701-b12c-02a209ff0ff7\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "0: jdbc:hive2://> \n",
      "0: jdbc:hive2://> -- Tabla 1: Datos de Steam\n",
      "0: jdbc:hive2://> CREATE EXTERNAL TABLE steam_spy_data (\n",
      ". . . . . . . . >     appid INT,\n",
      ". . . . . . . . >     name STRING,\n",
      ". . . . . . . . >     developer STRING,\n",
      ". . . . . . . . >     publisher STRING,\n",
      ". . . . . . . . >     positive INT,\n",
      ". . . . . . . . >     negative INT,\n",
      ". . . . . . . . >     owners_range STRING,\n",
      ". . . . . . . . >     average_playtime_total INT,\n",
      ". . . . . . . . >     average_playtime_2weeks INT,\n",
      ". . . . . . . . >     median_playtime_total INT,\n",
      ". . . . . . . . >     median_playtime_2weeks INT,\n",
      ". . . . . . . . >     price_usd DOUBLE,\n",
      ". . . . . . . . >     initialprice_usd DOUBLE,\n",
      ". . . . . . . . >     discount_percent DOUBLE,\n",
      ". . . . . . . . >     peak_current_players_yesterday INT,\n",
      ". . . . . . . . >     estimated_owners BIGINT\n",
      ". . . . . . . . > )\n",
      ". . . . . . . . > ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
      ". . . . . . . . > WITH SERDEPROPERTIES (\n",
      ". . . . . . . . >    \"separatorChar\" = \",\",\n",
      ". . . . . . . . >    \"quoteChar\"     = \"\\\"\",\n",
      ". . . . . . . . >    \"escapeChar\"    = \"\\\\\"\n",
      ". . . . . . . . > )\n",
      ". . . . . . . . > STORED AS TEXTFILE\n",
      ". . . . . . . . > LOCATION '/videojuegos/steam'\n",
      ". . . . . . . . > TBLPROPERTIES ('skip.header.line.count'='1');\n",
      "No rows affected (1.078 seconds)\n",
      "0: jdbc:hive2://> \n",
      "0: jdbc:hive2://> -- Tabla 2: Datos de Metacritic\n",
      "0: jdbc:hive2://> CREATE EXTERNAL TABLE metacritic_games (\n",
      ". . . . . . . . >     name STRING,\n",
      ". . . . . . . . >     release_date STRING,\n",
      ". . . . . . . . >     rating STRING,\n",
      ". . . . . . . . >     description STRING,\n",
      ". . . . . . . . >     score INT\n",
      ". . . . . . . . > )\n",
      ". . . . . . . . > ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
      ". . . . . . . . > WITH SERDEPROPERTIES (\n",
      ". . . . . . . . >    \"separatorChar\" = \",\",\n",
      ". . . . . . . . >    \"quoteChar\"     = \"\\\"\",\n",
      ". . . . . . . . >    \"escapeChar\"    = \"\\\\\"\n",
      ". . . . . . . . > )\n",
      ". . . . . . . . > STORED AS TEXTFILE\n",
      ". . . . . . . . > LOCATION '/videojuegos/metacritic'\n",
      ". . . . . . . . > TBLPROPERTIES ('skip.header.line.count'='1');\n",
      "No rows affected (0.042 seconds)\n",
      "0: jdbc:hive2://> \n",
      "0: jdbc:hive2://> Closing: 0: jdbc:hive2://\n"
     ]
    }
   ],
   "source": [
    "!beeline -u \"jdbc:hive2://\" -f crear-tablas-videojuegos.hql 2>&1 | grep -vE \"SLF4J|WARN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta 1\n",
    "Esta consulta agrupa los juegos por su clasificación de edad (Rating de la ESRB: 'M' para Mature, 'T' para Teen, etc.) y calcula cuánto tiempo pasan los usuarios jugándolos en promedio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = c72f25fb-1c29-4080-82f6-45edd6af47d1\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "No Stats for default@metacritic_games, Columns: name, rating\n",
      "No Stats for default@steam_spy_data, Columns: name, average_playtime_total\n",
      "Query ID = root_20251120101449_9cc94b4d-182e-4712-9e58-114746b6dce0\n",
      "Total jobs = 3\n",
      "Launching Job 1 out of 3\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1763631857148_0012, Tracking URL = http://yarnmanager:8088/proxy/application_1763631857148_0012/\n",
      "Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1763631857148_0012\n",
      "Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1\n",
      "2025-11-20 10:14:58,548 Stage-1 map = 0%,  reduce = 0%\n",
      "2025-11-20 10:15:04,694 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 9.95 sec\n",
      "2025-11-20 10:15:08,769 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 11.98 sec\n",
      "MapReduce Total cumulative CPU time: 11 seconds 980 msec\n",
      "Ended Job = job_1763631857148_0012\n",
      "Launching Job 2 out of 3\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1763631857148_0013, Tracking URL = http://yarnmanager:8088/proxy/application_1763631857148_0013/\n",
      "Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1763631857148_0013\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2025-11-20 10:15:19,327 Stage-2 map = 0%,  reduce = 0%\n",
      "2025-11-20 10:15:25,421 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.14 sec\n",
      "2025-11-20 10:15:31,528 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.35 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 350 msec\n",
      "Ended Job = job_1763631857148_0013\n",
      "Launching Job 3 out of 3\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1763631857148_0014, Tracking URL = http://yarnmanager:8088/proxy/application_1763631857148_0014/\n",
      "Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1763631857148_0014\n",
      "Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 1\n",
      "2025-11-20 10:15:42,047 Stage-3 map = 0%,  reduce = 0%\n",
      "2025-11-20 10:15:47,119 Stage-3 map = 100%,  reduce = 0%, Cumulative CPU 1.06 sec\n",
      "2025-11-20 10:15:53,209 Stage-3 map = 100%,  reduce = 100%, Cumulative CPU 2.29 sec\n",
      "MapReduce Total cumulative CPU time: 2 seconds 290 msec\n",
      "Ended Job = job_1763631857148_0014\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 11.98 sec   HDFS Read: 16370351 HDFS Write: 467 HDFS EC Read: 0 SUCCESS\n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 3.35 sec   HDFS Read: 10476 HDFS Write: 419 HDFS EC Read: 0 SUCCESS\n",
      "Stage-Stage-3: Map: 1  Reduce: 1   Cumulative CPU: 2.29 sec   HDFS Read: 9527 HDFS Write: 390 HDFS EC Read: 0 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 17 seconds 620 msec\n",
      "+----------------------------------------------------+-------------------------+\n",
      "|                 clasificacion_edad                 | promedio_minutos_juego  |\n",
      "+----------------------------------------------------+-------------------------+\n",
      "| RP                                                 | 3558.67                 |\n",
      "| E                                                  | 2450.77                 |\n",
      "| T                                                  | 1384.13                 |\n",
      "| M                                                  | 1318.79                 |\n",
      "|  follows the spell-binding plot from \"The Grand Magic Games\" story arc -- enabling gamers to re-live | 1014.00                 |\n",
      "| E10+                                               | 980.71                  |\n",
      "| N/A                                                | 818.05                  |\n",
      "| AO                                                 | 259.00                  |\n",
      "| Rating                                             | 0.00                    |\n",
      "+----------------------------------------------------+-------------------------+\n",
      "9 rows selected (65.889 seconds)\n",
      "Beeline version 4.0.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://\n"
     ]
    }
   ],
   "source": [
    "!beeline -u \"jdbc:hive2://\" -e \"SELECT m.rating AS Clasificacion_Edad, CAST(AVG(s.average_playtime_total) AS DECIMAL(10,2)) AS Promedio_Minutos_Juego FROM metacritic_games m JOIN steam_spy_data s ON (LOWER(TRIM(m.name)) = LOWER(TRIM(s.name))) WHERE m.rating IS NOT NULL AND m.rating != '' GROUP BY m.rating ORDER BY Promedio_Minutos_Juego DESC;\" 2>&1 | grep -vE \"SLF4J|WARN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consulta 2\n",
    "Esta consulta une ambas tablas por el nombre del juego para ver si una puntuación crítica alta (Metascore) se traduce en ventas altas en Steam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to jdbc:hive2://\n",
      "Hive Session ID = 067fb2ac-6301-4780-bcda-96cfa40817d5\n",
      "Connected to: Apache Hive (version 4.0.0)\n",
      "Driver: Hive JDBC (version 4.0.0)\n",
      "Transaction isolation: TRANSACTION_REPEATABLE_READ\n",
      "No Stats for default@metacritic_games, Columns: score, name\n",
      "No Stats for default@steam_spy_data, Columns: name, estimated_owners\n",
      "Query ID = root_20251120101600_742d001f-7b70-4b30-836c-4695aaedcb9f\n",
      "Total jobs = 2\n",
      "Launching Job 1 out of 2\n",
      "Number of reduce tasks not specified. Estimated from input data size: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1763631857148_0015, Tracking URL = http://yarnmanager:8088/proxy/application_1763631857148_0015/\n",
      "Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1763631857148_0015\n",
      "Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1\n",
      "2025-11-20 10:16:08,569 Stage-1 map = 0%,  reduce = 0%\n",
      "2025-11-20 10:16:15,751 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 11.72 sec\n",
      "2025-11-20 10:16:19,825 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 13.83 sec\n",
      "MapReduce Total cumulative CPU time: 13 seconds 830 msec\n",
      "Ended Job = job_1763631857148_0015\n",
      "Launching Job 2 out of 2\n",
      "Number of reduce tasks determined at compile time: 1\n",
      "In order to change the average load for a reducer (in bytes):\n",
      "  set hive.exec.reducers.bytes.per.reducer=<number>\n",
      "In order to limit the maximum number of reducers:\n",
      "  set hive.exec.reducers.max=<number>\n",
      "In order to set a constant number of reducers:\n",
      "  set mapreduce.job.reduces=<number>\n",
      "Starting Job = job_1763631857148_0016, Tracking URL = http://yarnmanager:8088/proxy/application_1763631857148_0016/\n",
      "Kill Command = /usr/local/hadoop/bin/mapred job  -kill job_1763631857148_0016\n",
      "Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1\n",
      "2025-11-20 10:16:30,369 Stage-2 map = 0%,  reduce = 0%\n",
      "2025-11-20 10:16:35,449 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.58 sec\n",
      "2025-11-20 10:16:42,558 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 3.03 sec\n",
      "MapReduce Total cumulative CPU time: 3 seconds 30 msec\n",
      "Ended Job = job_1763631857148_0016\n",
      "MapReduce Jobs Launched: \n",
      "Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 13.83 sec   HDFS Read: 16369798 HDFS Write: 99181 HDFS EC Read: 0 SUCCESS\n",
      "Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 3.03 sec   HDFS Read: 108368 HDFS Write: 1763 HDFS EC Read: 0 SUCCESS\n",
      "Total MapReduce CPU Time Spent: 16 seconds 860 msec\n",
      "+------------------+----------------------------------------------------+-------------------------+\n",
      "|      juego       |                     metascore                      | propietarios_estimados  |\n",
      "+------------------+----------------------------------------------------+-------------------------+\n",
      "| Name             | score                                              | estimated_owners        |\n",
      "| BioShock         | monster-closet AIs\" and static worlds, BioShock creates a living, unique and unpredictable FPS experience. After your plane crashes into icy uncharted waters, you discover a rusted bathysphere and descend into Rapture, a city hidden beneath the sea. Constructed as an idealistic society for a hand picked group of scientists, artists and industrialists, the idealism is no more. Now the city is littered with corpses, wildly powerful guardians roam the corridors as little girls loot the dead, and genetically mutated citizens ambush you at every turn. Take control of your world by hacking mechanical devices, commandeering security turrets and crafting unique items critical to your very survival. Upgrade your weapons with ionic gels, explosives and toxins to customize them to the enemy and environment. Genetically modify your body through dozens of Plasmid Stations scattered throughout the city, empowering you with fantastic and often grotesque abilities. Explore a living world powered by Ecological A.I., where the inhabitants have interesting and consequential relationships with one another that impact your gameplay experience. Experience truly next generation graphics that vividly illustrate the forlorn art deco city, highlighted by the most detailed and realistic water effects ever developed in a video game. Make meaningful choices and mature decisions, ultimately culminating in the grand question: do you exploit the innocent survivors of Rapture...or save them? [2K Games] | 3500000                 |\n",
      "| Baldur's Gate 3  | 96                                                 | 35000000                |\n",
      "| Half-Life        | 96                                                 | 15000000                |\n",
      "| Half-Life 2      | 96                                                 | 15000000                |\n",
      "+------------------+----------------------------------------------------+-------------------------+\n",
      "5 rows selected (43.839 seconds)\n",
      "Beeline version 4.0.0 by Apache Hive\n",
      "Closing: 0: jdbc:hive2://\n"
     ]
    }
   ],
   "source": [
    "!beeline -u \"jdbc:hive2://\"  -e \"SELECT m.name AS Juego, m.score AS Metascore, s.estimated_owners AS Propietarios_Estimados FROM metacritic_games m JOIN steam_spy_data s ON (LOWER(TRIM(m.name)) = LOWER(TRIM(s.name))) WHERE m.score IS NOT NULL ORDER BY m.score DESC LIMIT 5;\" 2>&1 | grep -vE \"SLF4J|WARN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
